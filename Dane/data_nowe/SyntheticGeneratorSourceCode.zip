{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============\n",
    "#Import Libraries\n",
    "#==============\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from random import randrange\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============\n",
    "#Constants\n",
    "#==============\n",
    "\n",
    "seasons = ['Summer', 'Autumn', 'Winter', 'Spring']\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "#==============\n",
    "#Parameters\n",
    "#==============\n",
    "\n",
    "EWH_RANGE = [0,1]\n",
    "\n",
    "parameters = {}\n",
    "parameters = {\n",
    "    \"mode\": 'real', # real = /conservative/nominal\n",
    "    \"considered_weeks\": 4, # number of weeks to read in from Input Data\n",
    "    \"desired_weeks\":1, # number of weeks to generate for Output Data\n",
    "    \n",
    "    \"differentiated_days\": 'yes', # 'yes' to include this data set/ 'no' to leave out\n",
    "    \"semi-differentiated_days\": 'yes', # 'yes' to include this data set/ 'no' to leave out\n",
    "    \"undifferentiated_days\": 'yes', # 'yes' to include this data set/ 'no' to leave out\n",
    "    \n",
    "    \"max_time_clusters\":7, # Time cluster limit\n",
    "    \"max_volume_clusters\":3, # Voume cluster limit\n",
    "    \"elbow_tolerance_time\":0.01, # Time clustering tolerance, smaller value = more clusters\n",
    "    \"elbow_tolerance_volume\":0.2, # Volume clustering tolerance, smaller value = more clusters\n",
    "    \"impulse_usages\":0, # 1 = all water events will have duration of 1 minute/ 0 = normal\n",
    "}\n",
    "\n",
    "# Read directory\n",
    "\n",
    "read_directory_part1 = 'Source Data/ewh_profile['\n",
    "read_directory_part2 = '].csv'\n",
    "\n",
    "# Save directory\n",
    "\n",
    "save_directory_part1 = 'Final Data/' + parameters[\"mode\"] + '/synthetic_profile['\n",
    "save_directory_part2 = '].csv'\n",
    "\n",
    "# Generate indices for weekdays and weekends:\n",
    "\n",
    "semi_differentiated_index = {}\n",
    "semi_differentiated_index['weekdays'] = [0,1,2,3,4]\n",
    "semi_differentiated_index['weekends'] = [5,6]\n",
    "for i in range(parameters[\"considered_weeks\"]-1):\n",
    "    for j in range(5):\n",
    "        semi_differentiated_index['weekdays'].append(semi_differentiated_index['weekdays'][-5]+7)\n",
    "    for j in range(2):\n",
    "        semi_differentiated_index['weekends'].append(semi_differentiated_index['weekends'][-2]+7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_data(ewh_id):\n",
    "    read_data = pd.read_csv(read_directory_part1 + str(ewh_id) + read_directory_part2, sep=',',header=None, na_values = '-', skiprows = 1)\n",
    "    source_data = {}\n",
    "    for s in range(len(seasons)):\n",
    "        source_data[seasons[s]] = list(read_data[4*s+1])\n",
    "    return source_data\n",
    "\n",
    "def get_flow_data(source_data):\n",
    "    flow_data = {}\n",
    "    if(parameters[\"differentiated_days\"] == 'yes'):\n",
    "        for d in range(len(days)):\n",
    "            flow_data[days[d]] = {}\n",
    "            for w in range(parameters[\"considered_weeks\"]):\n",
    "                flow_data[days[d]][w] = find_flow(source_data[(w*1440*7 + d*1440):(w*1440*7 + (d+1)*1440)])\n",
    "    if(parameters[\"semi-differentiated_days\"] == 'yes'):\n",
    "        flow_data['weekdays'] = {}\n",
    "        for d in range(len(semi_differentiated_index['weekdays'])):\n",
    "            flow_data['weekdays'][d] = find_flow(source_data[(semi_differentiated_index['weekdays'][d]*1440):((semi_differentiated_index['weekdays'][d]+1)*1440)])\n",
    "        flow_data['weekends'] = {}    \n",
    "        for d in range(len(semi_differentiated_index['weekends'])):    \n",
    "            flow_data['weekends'][d] = find_flow(source_data[(semi_differentiated_index['weekends'][d]*1440):((semi_differentiated_index['weekends'][d]+1)*1440)])\n",
    "    if(parameters[\"undifferentiated_days\"] == 'yes'):\n",
    "        flow_data['all'] = {}\n",
    "        for d in range(7*parameters[\"considered_weeks\"]):\n",
    "            flow_data['all'][d] = find_flow(source_data[(d*1440):((d+1)*1440)])\n",
    "    return flow_data\n",
    "\n",
    "def find_flow(source_data):\n",
    "    flows = {}\n",
    "    flows['time_bins'] = []\n",
    "    flows['flow_rate'] = []\n",
    "    for t in range(len(source_data)):\n",
    "        if(source_data[t] > 0):\n",
    "            flows['time_bins'].append(t)\n",
    "            flows['flow_rate'].append(source_data[t])\n",
    "    return flows\n",
    "\n",
    "def get_cluster_data(flow_data):\n",
    "    cluster_data = {}\n",
    "    if(parameters[\"differentiated_days\"] == 'yes'):\n",
    "        for d in range(len(days)):\n",
    "            cluster_data[days[d]] = {}\n",
    "            temp_cluster_data = {}\n",
    "            temp_cluster_data['time_bins'] = []\n",
    "            temp_cluster_data['flow_rate'] = []\n",
    "            for w in range(parameters[\"considered_weeks\"]):\n",
    "                temp_cluster_data['time_bins'].extend(flow_data[days[d]][w]['time_bins'])\n",
    "                temp_cluster_data['flow_rate'].extend(flow_data[days[d]][w]['flow_rate'])\n",
    "            cluster_data[days[d]] = get_clusters(cluster_data[days[d]], temp_cluster_data, \"differentiated_days\", flow_data[days[d]])\n",
    "    if(parameters[\"semi-differentiated_days\"] == 'yes'):\n",
    "            cluster_data['weekdays'] = {}\n",
    "            temp_cluster_data = {}\n",
    "            temp_cluster_data['time_bins'] = []\n",
    "            temp_cluster_data['flow_rate'] = []\n",
    "            for w in range(len(semi_differentiated_index['weekdays'])):\n",
    "                temp_cluster_data['time_bins'].extend(flow_data['weekdays'][w]['time_bins'])\n",
    "                temp_cluster_data['flow_rate'].extend(flow_data['weekdays'][w]['flow_rate'])\n",
    "            cluster_data['weekdays'] = get_clusters(cluster_data['weekdays'], temp_cluster_data, \"semi-differentiated_days1\", flow_data['weekdays'])\n",
    "            cluster_data['weekends'] = {}\n",
    "            temp_cluster_data = {}\n",
    "            temp_cluster_data['time_bins'] = []\n",
    "            temp_cluster_data['flow_rate'] = []\n",
    "            for w in range(len(semi_differentiated_index['weekends'])):\n",
    "                temp_cluster_data['time_bins'].extend(flow_data['weekends'][w]['time_bins'])\n",
    "                temp_cluster_data['flow_rate'].extend(flow_data['weekends'][w]['flow_rate'])\n",
    "            cluster_data['weekends'] = get_clusters(cluster_data['weekends'], temp_cluster_data, \"semi-differentiated_days2\", flow_data['weekends'])\n",
    "    if(parameters[\"undifferentiated_days\"] == 'yes'):\n",
    "            cluster_data['all'] = {}\n",
    "            temp_cluster_data = {}\n",
    "            temp_cluster_data['time_bins'] = []\n",
    "            temp_cluster_data['flow_rate'] = []\n",
    "            for w in range(len(list(range(7*parameters[\"considered_weeks\"])))):\n",
    "                temp_cluster_data['time_bins'].extend(flow_data['all'][w]['time_bins'])\n",
    "                temp_cluster_data['flow_rate'].extend(flow_data['all'][w]['flow_rate'])\n",
    "            cluster_data['all'] = get_clusters(cluster_data['all'], temp_cluster_data, \"undifferentiated_days\", flow_data['all'])\n",
    "\n",
    "    return cluster_data\n",
    "    \n",
    "def get_clusters(cluster_data, temp_cluster_data, day_type, flow_data):\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    cluster_data['time_cluster_amount'] = elbow_method(temp_cluster_data, parameters[\"max_time_clusters\"], 'time_bins', parameters[\"elbow_tolerance_time\"])\n",
    "    cluster_data['time_cluster'] = {}\n",
    "    cluster_data['time_cluster']['event_starts'] = {}\n",
    "    cluster_data['time_cluster']['event_volumes'] = {}\n",
    "    cluster_data['time_cluster']['event_flow_rates'] = {}\n",
    "    cluster_data['time_cluster']['time_boundaries'] = {}\n",
    "    cluster_data['time_cluster']['null_probability'] = {}\n",
    "    cluster_data['time_cluster']['start_model'] = {}\n",
    "    \n",
    "    if(cluster_data['time_cluster_amount'] > 0):\n",
    "        cluster_data['time_cluster']['time_bins'], cluster_data['time_cluster']['flow_rate'] = cluster(cluster_data['time_cluster_amount'], temp_cluster_data, 'time_bins')\n",
    "    \n",
    "    for t in range(cluster_data['time_cluster_amount']):\n",
    "        cluster_data['time_cluster']['time_boundaries'][t] = [min(cluster_data['time_cluster']['time_bins'][t]),max(cluster_data['time_cluster']['time_bins'][t])]\n",
    "        cluster_data['time_cluster']['event_starts'][t], cluster_data['time_cluster']['event_volumes'][t], cluster_data['time_cluster']['event_flow_rates'][t] = get_event_parameters(flow_data, cluster_data['time_cluster']['time_boundaries'][t]) \n",
    "        cluster_data['time_cluster']['start_model'][t] = get_gauss(cluster_data['time_cluster']['time_bins'][t])\n",
    "    cluster_data['flow_cluster'] = {}\n",
    "    cluster_data['flow_cluster']['flow_boundaries'] = {}\n",
    "    cluster_data['flow_cluster']['volume_boundaries'] = {}\n",
    "    cluster_data['flow_cluster']['volume'] = {}\n",
    "    cluster_data['flow_cluster']['flow_rate'] = {}\n",
    "    cluster_data['flow_cluster']['probability'] = {}\n",
    "    cluster_data['flow_cluster']['covariance'] = {}\n",
    "    cluster_data['flow_cluster']['mean'] = {}\n",
    "    \n",
    "    for t in range(cluster_data['time_cluster_amount']):\n",
    "        new_temp_cluster = {}\n",
    "        new_temp_cluster['volume'] = cluster_data['time_cluster']['event_volumes'][t]\n",
    "        new_temp_cluster['flow_rate'] = cluster_data['time_cluster']['event_flow_rates'][t]\n",
    "        cluster_data['flow_cluster']['flow_boundaries'][t] = {}\n",
    "        cluster_data['flow_cluster']['volume_boundaries'][t] = {}\n",
    "        cluster_data['flow_cluster']['probability'][t] = {}\n",
    "        cluster_data['flow_cluster']['covariance'][t] = {}\n",
    "        cluster_data['flow_cluster']['mean'][t] = {}\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            cluster_data['flow_cluster_amount'] = elbow_method_2(new_temp_cluster, parameters[\"max_volume_clusters\"], 'volume', parameters[\"elbow_tolerance_volume\"])\n",
    "        if(cluster_data['flow_cluster_amount'] > 0):\n",
    "            cluster_data['flow_cluster']['volume'][t], cluster_data['flow_cluster']['flow_rate'][t] = cluster_2(cluster_data['flow_cluster_amount'], new_temp_cluster, 'volume')\n",
    "        temp_probability = 0\n",
    "        for f in range(cluster_data['flow_cluster_amount']):\n",
    "            cluster_data['flow_cluster']['flow_boundaries'][t][f] = [min(cluster_data['flow_cluster']['flow_rate'][t][f]),max(cluster_data['flow_cluster']['flow_rate'][t][f])]\n",
    "            cluster_data['flow_cluster']['volume_boundaries'][t][f] = [min(cluster_data['flow_cluster']['volume'][t][f]),max(cluster_data['flow_cluster']['volume'][t][f])]\n",
    "            if(len(cluster_data['flow_cluster']['flow_rate'][t][f]) < 2):\n",
    "                cluster_data['flow_cluster']['covariance'][t][f] = [[0,0],[0,0]]\n",
    "                cluster_data['flow_cluster']['mean'][t][f] = [cluster_data['flow_cluster']['flow_rate'][t][f][0],cluster_data['flow_cluster']['volume'][t][f][0]]\n",
    "            else:\n",
    "                cluster_data['flow_cluster']['covariance'][t][f] = np.cov(cluster_data['flow_cluster']['flow_rate'][t][f], cluster_data['flow_cluster']['volume'][t][f])*(0.1)\n",
    "                cluster_data['flow_cluster']['mean'][t][f] = [np.mean(cluster_data['flow_cluster']['flow_rate'][t][f]), np.mean(cluster_data['flow_cluster']['volume'][t][f])]\n",
    "            cluster_data['flow_cluster']['probability'][t][f] = get_cluster_probability(day_type, flow_data, cluster_data['flow_cluster']['flow_boundaries'][t][f], cluster_data['flow_cluster']['volume_boundaries'][t][f], cluster_data['time_cluster']['time_boundaries'][t])    \n",
    "            temp_probability += cluster_data['flow_cluster']['probability'][t][f]\n",
    "        cluster_data['time_cluster']['null_probability'][t] = 100 - temp_probability\n",
    "    return cluster_data\n",
    "\n",
    "def get_event_parameters(flow_rates, time_boundaries):\n",
    "    event_starts = []\n",
    "    event_volumes = []\n",
    "    event_flow_rates = []\n",
    "    for f in range(len(flow_rates)):\n",
    "        temp_event_flows = []\n",
    "        for x in range(len(flow_rates[f]['time_bins'])):\n",
    "            if(time_boundaries[0] <= flow_rates[f]['time_bins'][x] <= time_boundaries[1]):\n",
    "                event_starts.append(flow_rates[f]['time_bins'][x])\n",
    "                temp_event_flows.append(flow_rates[f]['flow_rate'][x])\n",
    "        if(len(temp_event_flows) > 0):\n",
    "            event_volumes.append(sum(temp_event_flows))\n",
    "            event_flow_rates.append(sum(temp_event_flows)/len(temp_event_flows))\n",
    "    return event_starts, event_volumes, event_flow_rates\n",
    "\n",
    "def get_cluster_probability(day_type, flow_rates, flow_boundaries, volume_boundaries, time_boundaries):\n",
    "    numerator = 0\n",
    "    if(day_type == \"differentiated_days\"):denominator = parameters[\"considered_weeks\"]\n",
    "    elif(day_type == \"semi-differentiated_days1\"):denominator = parameters[\"considered_weeks\"]*5\n",
    "    elif(day_type == \"semi-differentiated_days2\"):denominator = parameters[\"considered_weeks\"]*2\n",
    "    elif(day_type == \"undifferentiated_days\"):denominator = parameters[\"considered_weeks\"]*7\n",
    "        \n",
    "    event_volumes = []\n",
    "    event_flow_rates = []\n",
    "\n",
    "    for f in range(len(flow_rates)):\n",
    "        found = 0\n",
    "        temp_flows = []\n",
    "        for g in range(len(flow_rates[f]['time_bins'])):\n",
    "            if(time_boundaries[0] <= flow_rates[f]['time_bins'][g] <= time_boundaries[1]):\n",
    "                temp_flows.append(flow_rates[f]['flow_rate'][g])\n",
    "        if(len(temp_flows) > 0):\n",
    "            event_volumes.append(sum(temp_flows))\n",
    "            event_flow_rates.append(sum(temp_flows)/len(temp_flows))\n",
    "    for i in range(len(event_volumes)):\n",
    "        if(flow_boundaries[0] <= event_flow_rates[i] <= flow_boundaries[1]):\n",
    "            if(volume_boundaries[0] <= event_volumes[i] <= volume_boundaries[1]):\n",
    "                numerator += 1\n",
    "    return (numerator/denominator)*100\n",
    "\n",
    "def get_centroid(time_boundaries, flow_rates):\n",
    "    centroids = []\n",
    "    for f in range(len(flow_rates)):\n",
    "        cluster_flow_rates = []\n",
    "        for x in range(len(flow_rates[f]['time_bins'])):\n",
    "            if(time_boundaries[0] <= flow_rates[f]['time_bins'][x] <= time_boundaries[1]):\n",
    "                cluster_flow_rates.append(flow_rates[f]['flow_rate'][x])\n",
    "        if(len(cluster_flow_rates) > 0):\n",
    "            if(len(cluster_flow_rates)%2 == 0): #if even\n",
    "                median = np.percentile(cluster_flow_rates, 50)\n",
    "                middle_left = cluster_flow_rates[int(len(cluster_flow_rates)/2-1)]\n",
    "                middle_right = cluster_flow_rates[int(len(cluster_flow_rates)/2)]\n",
    "                if(median - middle_left <= middle_right - median):\n",
    "                    centroids.append(middle_left)\n",
    "                else:\n",
    "                    centroids.append(middle_right)\n",
    "            else: #if odd\n",
    "                centroids.append(np.percentile(cluster_flow_rates, 50))\n",
    "    return centroids\n",
    "\n",
    "def get_gauss(data):\n",
    "    new_data = {}\n",
    "    if(len(data) > 1):\n",
    "        sample_mean = np.mean(data)\n",
    "        sample_std = np.std(data)\n",
    "        dist = norm(sample_mean, sample_std)\n",
    "        new_data['values'] = [new_data['values'] for new_data['values'] in range(min(data)-100, max(data)+100)]\n",
    "        temp_values = new_data['values'].copy()\n",
    "        probabilities = [dist.pdf(temp_values) for temp_values in temp_values]\n",
    "        new_data['cum_probability'] = []\n",
    "        current = 0\n",
    "        for i in range(len(probabilities)):\n",
    "            current += probabilities[i]\n",
    "            new_data['cum_probability'].append(current)\n",
    "        new_data['cum_probability'].append(1)\n",
    "    else:\n",
    "        new_data['values'] = [data[0]]\n",
    "        new_data['cum_probability'] = [0,1]\n",
    "    return new_data\n",
    "        \n",
    "def pick_from_gauss(data, time_bins):\n",
    "    valid = 0\n",
    "    while(valid == 0):\n",
    "        x = random.uniform(0, 1)\n",
    "        picked_sample = min(time_bins)\n",
    "        for j in range(len(data['cum_probability'])-1):\n",
    "            if(data['cum_probability'][j] <= x < data['cum_probability'][j+1]):\n",
    "                picked_sample = data['values'][j]\n",
    "        if(min(time_bins) <= picked_sample <= max(time_bins)):valid = 1\n",
    "    return picked_sample\n",
    "\n",
    "def elbow_method(cluster_data, max_clusters, cluster_type, elbow_tolerance):\n",
    "    unique_length = len([list(x) for x in set(tuple(x) for x in list_to_coordinates(cluster_data['time_bins'], cluster_data['flow_rate']))])\n",
    "    if(unique_length >= 1):\n",
    "        Y = np.array(cluster_data[cluster_type]).reshape(-1,1)\n",
    "        if(unique_length < max_clusters):\n",
    "            kmeans = [KMeans(n_clusters=i) for i in range(1, unique_length+1)]\n",
    "        else:\n",
    "            kmeans = [KMeans(n_clusters=i) for i in range(1, max_clusters+1)]\n",
    "        score = [kmeans[i].fit(Y).score(Y) for i in range(len(kmeans))]\n",
    "        new_score = []\n",
    "        if(abs(score[0]) == 0):\n",
    "            return 1\n",
    "        else:\n",
    "            for i in range(len(score)):\n",
    "                new_score.append(abs(score[i])/abs(score[0]))\n",
    "            delta = new_score[0]\n",
    "            i = 0\n",
    "            while(delta > elbow_tolerance and i < unique_length-1 and i < max_clusters-1):\n",
    "                i += 1\n",
    "                delta = abs(new_score[i-1]) - abs(new_score[i])\n",
    "            if(i == 0):i = 1\n",
    "            return i\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def elbow_method_2(cluster_data, max_clusters, cluster_type, elbow_tolerance):\n",
    "    unique_length = len([list(x) for x in set(tuple(x) for x in list_to_coordinates(cluster_data['volume'], cluster_data['flow_rate']))])\n",
    "    if(unique_length >= 1):\n",
    "        Y = np.array(cluster_data[cluster_type]).reshape(-1,1)\n",
    "        if(unique_length < max_clusters):\n",
    "            kmeans = [KMeans(n_clusters=i) for i in range(1, unique_length+1)]\n",
    "        else:\n",
    "            kmeans = [KMeans(n_clusters=i) for i in range(1, max_clusters+1)]\n",
    "        score = [kmeans[i].fit(Y).score(Y) for i in range(len(kmeans))]\n",
    "        new_score = []\n",
    "        if(abs(score[0]) == 0):\n",
    "            return 1\n",
    "        else:\n",
    "            for i in range(len(score)):\n",
    "                new_score.append(abs(score[i])/abs(score[0]))\n",
    "            delta = new_score[0]\n",
    "            i = 0\n",
    "            while(delta > elbow_tolerance and i < unique_length-1 and i < max_clusters-1):\n",
    "                i += 1\n",
    "                delta = abs(new_score[i-1]) - abs(new_score[i])\n",
    "            if(i == 0):i = 1\n",
    "            return i\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def list_to_coordinates(list_1, list_2):\n",
    "    return [list(z) for z in zip(list_1, list_2)]\n",
    "\n",
    "def cluster(cluster_amount, cluster_data, cluster_type):\n",
    "    old_data = {}\n",
    "    old_data['time_bins'] = {}\n",
    "    old_data['flow_rate'] = {}\n",
    "    \n",
    "    Y = np.array(cluster_data[cluster_type]).reshape(-1,1)    \n",
    "    clustered_data = KMeans(cluster_amount, random_state = 0).fit(Y)\n",
    "    for i in range(cluster_amount):\n",
    "        old_data['time_bins'][i] = []\n",
    "        old_data['flow_rate'][i] = []\n",
    "    for j in range(len(clustered_data.labels_)):\n",
    "        old_data['time_bins'][clustered_data.labels_[j]].append(cluster_data['time_bins'][j])\n",
    "        old_data['flow_rate'][clustered_data.labels_[j]].append(cluster_data['flow_rate'][j])\n",
    "    first_samples = []\n",
    "    for i in range(cluster_amount):\n",
    "        first_samples.append(min(old_data[cluster_type][i]))\n",
    "    sort_list = sorted(range(len(first_samples)), key=lambda k: first_samples[k])\n",
    "    data = {}\n",
    "    data['time_bins'] = {}\n",
    "    data['flow_rate'] = {}\n",
    "    for i in range(len(sort_list)):\n",
    "        data['time_bins'][i] = old_data['time_bins'][sort_list[i]]\n",
    "        data['flow_rate'][i] = old_data['flow_rate'][sort_list[i]]\n",
    "    return data['time_bins'], data['flow_rate']\n",
    "\n",
    "def cluster_2(cluster_amount, cluster_data, cluster_type):\n",
    "    old_data = {}\n",
    "    old_data['volume'] = {}\n",
    "    old_data['flow_rate'] = {}\n",
    "    \n",
    "    Y = np.array(cluster_data[cluster_type]).reshape(-1,1)    \n",
    "    clustered_data = KMeans(cluster_amount, random_state = 0).fit(Y)\n",
    "    for i in range(cluster_amount):\n",
    "        old_data['volume'][i] = []\n",
    "        old_data['flow_rate'][i] = []\n",
    "    for j in range(len(clustered_data.labels_)):\n",
    "        old_data['volume'][clustered_data.labels_[j]].append(cluster_data['volume'][j])\n",
    "        old_data['flow_rate'][clustered_data.labels_[j]].append(cluster_data['flow_rate'][j])\n",
    "    first_samples = []\n",
    "    for i in range(cluster_amount):\n",
    "        first_samples.append(min(old_data[cluster_type][i]))\n",
    "    sort_list = sorted(range(len(first_samples)), key=lambda k: first_samples[k])\n",
    "    data = {}\n",
    "    data['volume'] = {}\n",
    "    data['flow_rate'] = {}\n",
    "    for i in range(len(sort_list)):\n",
    "        data['volume'][i] = old_data['volume'][sort_list[i]]\n",
    "        data['flow_rate'][i] = old_data['flow_rate'][sort_list[i]]\n",
    "    return data['volume'], data['flow_rate']\n",
    "\n",
    "def get_event_data(cluster_data):\n",
    "    event_data = {}\n",
    "    if(parameters[\"differentiated_days\"] == 'yes'):\n",
    "        for d in range(len(days)):\n",
    "            event_data[days[d]] = get_events(cluster_data[days[d]], parameters[\"desired_weeks\"])\n",
    "    if(parameters[\"semi-differentiated_days\"] == 'yes'):\n",
    "        for d in range(5):\n",
    "            event_data['weekday_' + str(d)] = get_events(cluster_data['weekdays'], parameters[\"desired_weeks\"])\n",
    "        for d in range(5,7):\n",
    "            event_data['weekday_' + str(d)] = get_events(cluster_data['weekends'], parameters[\"desired_weeks\"])\n",
    "    if(parameters[\"undifferentiated_days\"] == 'yes'):\n",
    "        for d in range(len(days)):\n",
    "            event_data['all_' + str(d)] = get_events(cluster_data['all'], parameters[\"desired_weeks\"])\n",
    "    return event_data\n",
    "\n",
    "def get_events(cluster_data, day_amount):\n",
    "    events = {}\n",
    "    events['event_starts'] = {}\n",
    "    events['event_volumes'] = {}\n",
    "    events['event_flow_rates'] = {}\n",
    "    for w in range(day_amount):\n",
    "        events['event_starts'][w] = []\n",
    "        events['event_volumes'][w] = []\n",
    "        events['event_flow_rates'][w] = []\n",
    "        for t in range(cluster_data['time_cluster_amount']):\n",
    "            if(parameters[\"mode\"] == 'nominal'):\n",
    "                flow_event_occurence = pick_nominal_event(cluster_data, t)\n",
    "                events['event_starts'][w].append(int(np.percentile(cluster_data['time_cluster']['time_bins'][t], 50)))\n",
    "                events['event_volumes'][w].append(np.percentile(cluster_data['flow_cluster']['volume'][t][flow_event_occurence], 50))\n",
    "                events['event_flow_rates'][w].append(np.percentile(cluster_data['flow_cluster']['flow_rate'][t][flow_event_occurence], 50))\n",
    "            elif(parameters[\"mode\"] == 'conservative'):\n",
    "                events['event_starts'][w].append(int(np.percentile(cluster_data['time_cluster']['time_bins'][t], 0)))\n",
    "                volumes = list(filter(lambda num: num != 0, cluster_data['flow_cluster']['volume'][t][len(cluster_data['flow_cluster']['volume'][t])-1]))\n",
    "                events['event_volumes'][w].append(np.percentile(volumes, 100))\n",
    "                events['event_flow_rates'][w].append(np.percentile(cluster_data['flow_cluster']['flow_rate'][t][len(cluster_data['flow_cluster']['volume'][t])-1], 50))\n",
    "            elif(parameters[\"mode\"] == 'real'):       \n",
    "                flow_event_occurence = pick_random_event(cluster_data, t)\n",
    "                if(flow_event_occurence != -1):\n",
    "                    events['event_starts'][w].append(pick_from_gauss(cluster_data['time_cluster']['start_model'][t], cluster_data['time_cluster']['time_bins'][t]))\n",
    "                    event_volume, event_flow_rate = pick_from_fitted_gauss(cluster_data['flow_cluster']['covariance'][t][flow_event_occurence], cluster_data['flow_cluster']['mean'][t][flow_event_occurence])\n",
    "                    events['event_volumes'][w].append(event_volume)\n",
    "                    events['event_flow_rates'][w].append(event_flow_rate)\n",
    "    return events\n",
    "\n",
    "def pick_from_fitted_gauss(covariance, mean):\n",
    "    gauss = [[-1,-1]]\n",
    "    while(gauss[0][0]<=0 or gauss[0][1]<=0):\n",
    "        gauss = np.random.multivariate_normal(mean, covariance, 1)\n",
    "    return gauss[0][1], gauss[0][0]\n",
    "    \n",
    "def combine_volumes(event_volumes, volume_index):\n",
    "    volume = 0\n",
    "    temp_volume = []\n",
    "    for i in range(len(event_volumes)):\n",
    "        temp_volume.append(event_volumes[i][volume_index])\n",
    "        volume += event_volumes[i][volume_index]\n",
    "    return volume\n",
    "\n",
    "def pick_random_event(cluster_data, time_cluster):\n",
    "    \n",
    "    probability_list = [0]\n",
    "    for f in range(len(cluster_data['flow_cluster']['probability'][time_cluster])):\n",
    "        if(cluster_data['flow_cluster']['probability'][time_cluster][f] > 0):\n",
    "            probability_list.append(probability_list[-1] + cluster_data['flow_cluster']['probability'][time_cluster][f]/100)\n",
    "    x = random.uniform(0, 1)\n",
    "    chosen_flow_cluster = -1 #-1 means that no flow cluster occurs for the time slot\n",
    "    for f in range(len(probability_list)-1):\n",
    "        if(probability_list[f] <= x < probability_list[f+1]):chosen_flow_cluster = f\n",
    "    return chosen_flow_cluster \n",
    "\n",
    "def pick_nominal_event(cluster_data, time_cluster):\n",
    "    \n",
    "    probability_list = [0]\n",
    "    for f in range(len(cluster_data['flow_cluster']['probability'][time_cluster])):\n",
    "        if(cluster_data['flow_cluster']['probability'][time_cluster][f] > 0):\n",
    "            probability_list.append(probability_list[-1] + cluster_data['flow_cluster']['probability'][time_cluster][f]/100)\n",
    "    chosen_flow_cluster = -1 #-1 means that no flow cluster occurs for the time slot\n",
    "    x = 0\n",
    "    for f in range(len(probability_list)-1):\n",
    "        if((probability_list[f+1] - probability_list[f]) > x):\n",
    "            x = (probability_list[f+1] - probability_list[f])\n",
    "            chosen_flow_cluster = f\n",
    "    return chosen_flow_cluster \n",
    "\n",
    "def get_profile_data(event_data):\n",
    "    profile_data = {}\n",
    "    if(parameters[\"differentiated_days\"] == 'yes'):\n",
    "        profile_data[\"differentiated_days\"] = []\n",
    "        for w in range(parameters[\"desired_weeks\"]):\n",
    "            for d in range(len(days)):\n",
    "                profile_data[\"differentiated_days\"].extend(get_profile(event_data[days[d]]['event_starts'][w], event_data[days[d]]['event_volumes'][w], event_data[days[d]]['event_flow_rates'][w]))\n",
    "    if(parameters[\"semi-differentiated_days\"] == 'yes'):\n",
    "        profile_data[\"semi-differentiated_days\"] = []\n",
    "        for w in range(parameters[\"desired_weeks\"]):\n",
    "            for d in range(len(days)):\n",
    "                profile_data[\"semi-differentiated_days\"].extend(get_profile(event_data['weekday_' + str(d)]['event_starts'][w], event_data['weekday_' + str(d)]['event_volumes'][w], event_data['weekday_' + str(d)]['event_flow_rates'][w]))\n",
    "    if(parameters[\"undifferentiated_days\"] == 'yes'):\n",
    "        profile_data[\"undifferentiated_days\"] = []\n",
    "        for w in range(parameters[\"desired_weeks\"]):\n",
    "            for d in range(7):\n",
    "                profile_data[\"undifferentiated_days\"].extend(get_profile(event_data['all_' + str(d)]['event_starts'][w], event_data['all_' + str(d)]['event_volumes'][w], event_data['all_' + str(d)]['event_flow_rates'][w]))\n",
    "    return profile_data\n",
    "                \n",
    "def get_profile(event_starts, event_volumes, event_flow_rates):\n",
    "    day_profile = [0]*1440\n",
    "    for i in range(len(event_starts)):\n",
    "        if(parameters[\"impulse_usages\"] == 1):event_flow_rates[i] = 9999999\n",
    "        if(event_flow_rates[i] < 0.5):event_flow_rates[i] = 0.5    \n",
    "        duration = int(event_volumes[i]/event_flow_rates[i])\n",
    "        if(parameters[\"mode\"] == 'conservative'):\n",
    "            real_event_start = event_starts[i]\n",
    "        else:\n",
    "            real_event_start = event_starts[i] - int(duration/2)\n",
    "        remainder = event_volumes[i]%event_flow_rates[i]\n",
    "        if(real_event_start + duration > 1438):\n",
    "            shift_event = (real_event_start + duration) - 1438\n",
    "            real_event_start -= shift_event\n",
    "        for d in range(duration):\n",
    "            day_profile[real_event_start + d] += event_flow_rates[i]\n",
    "        if(remainder > 0):\n",
    "            day_profile[real_event_start + duration] += remainder\n",
    "    return day_profile\n",
    "\n",
    "    \n",
    "def save_profiles(profile_data, ewh_id):\n",
    "    save_dict = {}\n",
    "    if(parameters[\"differentiated_days\"] == 'yes'):\n",
    "        for s in range(len(seasons)):\n",
    "            save_dict[seasons[s] + '[differentiated_days]'] = profile_data[seasons[s]][\"differentiated_days\"]\n",
    "    if(parameters[\"semi-differentiated_days\"] == 'yes'):\n",
    "        for s in range(len(seasons)):\n",
    "            save_dict[seasons[s] + '[semi-differentiated_days]'] = profile_data[seasons[s]][\"semi-differentiated_days\"]\n",
    "    if(parameters[\"undifferentiated_days\"] == 'yes'):\n",
    "        for s in range(len(seasons)):\n",
    "            save_dict[seasons[s] + '[undifferentiated_days]'] = profile_data[seasons[s]][\"undifferentiated_days\"]\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    df.to_csv(save_directory_part1 + str(ewh_id) + save_directory_part2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "\n",
    "print('Reading Source Data...')\n",
    "source_data = {}\n",
    "for q in EWH_RANGE:\n",
    "    source_data[q] = get_source_data(q)\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "print('Extracting Flow Data...')\n",
    "flow_data = {}\n",
    "for q in EWH_RANGE:\n",
    "    flow_data[q] = {}\n",
    "    for s in range(len(seasons)):\n",
    "        flow_data[q][seasons[s]] = get_flow_data(source_data[q][seasons[s]])\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "print('Clustering Data...')\n",
    "cluster_data = {}\n",
    "for q in EWH_RANGE:\n",
    "    cluster_data[q] = {}\n",
    "    for s in range(len(seasons)):\n",
    "        cluster_data[q][seasons[s]] = get_cluster_data(flow_data[q][seasons[s]])\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "print('Generating Events...')\n",
    "event_data = {}\n",
    "for q in EWH_RANGE:\n",
    "    event_data[q] = {}\n",
    "    for s in range(len(seasons)):\n",
    "        event_data[q][seasons[s]] = get_event_data(cluster_data[q][seasons[s]])\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "print('Building Schedules...')\n",
    "profile_data = {}\n",
    "for q in EWH_RANGE:\n",
    "    profile_data[q] = {}\n",
    "    for s in range(len(seasons)):\n",
    "        profile_data[q][seasons[s]] = get_profile_data(event_data[q][seasons[s]])\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "print('Saving Profiles...')\n",
    "for q in EWH_RANGE:\n",
    "    save_profiles(profile_data[q], q)\n",
    "print('Finished!')\n",
    "\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
